{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df23382a",
   "metadata": {},
   "source": [
    "\n",
    "WHY USE SQRT (DIMENSIONS)\n",
    "\n",
    "REASON 1: For STABILITY IN LEARNING\n",
    "\n",
    "the softmax function is sensitive to the magnitudes od its inputs. when the inputs. when the inputs are very large, the differences between the exponential values of each input become much more pronounced. This causes softmax output to become peaky, where the highest value recieves almost all the probability mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd9400ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "tensor = torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f322e109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax result without scaling a tensor :  tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
      "softmax after scaling (tensor * 8) :  tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "softmax_result = torch.softmax(tensor, dim = -1)\n",
    "\n",
    "print(\"softmax result without scaling a tensor : \", softmax_result)\n",
    "\n",
    "# After applying softmax to scaled tensor\n",
    "scaled_tensor = tensor * 8\n",
    "scaled_softmax_result = torch.softmax(scaled_tensor, dim = -1)\n",
    "print(\"softmax after scaling (tensor * 8) : \", scaled_softmax_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2556bb78",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "\n",
    "In attention mechanism, particularly transformer, if the dot product between key and query vector become too large then attention scores can become very large. This result in a very sharp softmax distribution making the model overly confident in one particular key. such sharp distribution can make learning very unstable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c60789",
   "metadata": {},
   "source": [
    "BUT STILL WHY USE SQRT ?\n",
    "\n",
    "REASON 2: To make variance of dot product stable.\n",
    "\n",
    "The dot product of Q and K increases the variance because multiplying the two random numbers increases the variance.\n",
    "\n",
    "The increase in variance grows with dimension & dividing by Sqrt (dimension) Keep the variance to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3567cf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance without scaling for dims: 20 : 18.797747956168774\n",
      "variance after scaling for dim 20 : 0.9398873978084384\n",
      "variance without scaling for dims: 100 : 105.24283795051979\n",
      "variance after scaling for dim 100 : 1.0524283795051979\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# function to compute variance before and after scaling\n",
    "\n",
    "def compute_variance(dim, num_trials = 1000):\n",
    "    dot_products = []\n",
    "    scaled_dot_products = []\n",
    "\n",
    "\n",
    "    # Generate multiple random vectors and compute dot products\n",
    "\n",
    "    for _ in range(num_trials):\n",
    "        q = np.random.randn(dim)\n",
    "        k = np.random.randn(dim)\n",
    "\n",
    "\n",
    "        # compute dot product\n",
    "        dot_product = np.dot(q, k)\n",
    "        dot_products.append(dot_product)\n",
    "\n",
    "\n",
    "        scaled_dot_product = dot_product / np.sqrt(dim)\n",
    "        scaled_dot_products.append(scaled_dot_product)\n",
    "\n",
    "\n",
    "    non_scaled_var = np.var(dot_products)\n",
    "    print(f\"variance without scaling for dims: {dim} : {non_scaled_var}\")\n",
    "\n",
    "    scaled_var = np.var(scaled_dot_products)\n",
    "    print(f\"variance after scaling for dim {dim} : {scaled_var}\")\n",
    "\n",
    "\n",
    "\n",
    "compute_variance(20)\n",
    "compute_variance(100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
