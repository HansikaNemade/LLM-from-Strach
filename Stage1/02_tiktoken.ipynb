{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7475c046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version :  0.8.0\n",
      "[15496, 11, 347, 11401, 11, 2561, 345, 1498, 284, 10385, 428, 2420, 656, 3748, 32373, 50256, 818, 262, 19606, 8812, 2114, 286, 617, 34680, 27271, 220, 50256]\n",
      "[23155, 74, 1447, 35918]\n",
      "Awkird ire\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import importlib\n",
    "import importlib.metadata\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version : \", importlib.metadata.version(\"tiktoken\"))\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = \"Hello, BPE, Will you able to convert this text into unique IDs\"\n",
    "token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "text2 = text + \"<|endoftext|>\" + \"In the sunlight terraces of someunknownPlace <|endoftext|>\"\n",
    "token_ids = tokenizer.encode(text2, allowed_special={\"<|endoftext|>\"})\n",
    "print(token_ids)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "First, the <|endoftext|> token is assigned a relatively large token ID, namely 50256\n",
    "In fact, the BPE tokenizer, which is used to train models such as GPT2, GPT3, and the original model is used\n",
    "with <|endoftext|> being assigned the largest token ID\n",
    "\n",
    "Secondly, the BPE Tokenizer above encodes and decodes unknown words such as \"someunknowPace\" correctly\n",
    "\n",
    "The BPE tokenizer can handle any unknown word.How does it achieve this without using <|unk|> token ?\n",
    "The algorithm underlying BPE breaks down words that aren't in its predefined vocabulary into smaller sub words,\n",
    "that enables it to handle out-of-vocab words\n",
    "\n",
    "So thanks to BPE Algorithm, if the tokenizer encounters an unfamiliar word during tokenization, it can represent it \n",
    "as a sequence of subword token or characters\n",
    "\"\"\"\n",
    "\n",
    "token_ids = tokenizer.encode(\"Awkird ire\")\n",
    "print(token_ids)\n",
    "print(tokenizer.decode(token_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
