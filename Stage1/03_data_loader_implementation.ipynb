{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "401440df",
   "metadata": {},
   "source": [
    "**For the efficient data loader implementation, we will use pytorch build-in-dataset and dataloader classes**\n",
    "\n",
    "Step1: Tokenize the entire text\n",
    "\n",
    "Step2: use a sliding window to chunk the book into the overlaping sequence of max length\n",
    "\n",
    "Step3: Return the total number of rows in the dataset (no. token id in one batch)\n",
    "\n",
    "Step4: Return single row from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db43b39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import re\n",
    "import tiktoken\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2635af",
   "metadata": {},
   "source": [
    "\n",
    "\"\"\"\n",
    "The GPTDatasetV1 Class is listing 2.5 is based on Pytorch Dataset class\n",
    "It defines how individual rows are fetched dataset\n",
    "Each row consists of a number of token IDs (based on max length) assigned to input_chunk tensor\n",
    "The target chunk contains corresponding targets\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48872f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPTDatasetV1(Dataset) :\n",
    "  def __init__(self, tokenizer, text, context_window_size, stride):\n",
    "    self.input_ids = []\n",
    "    self.target_ids = []\n",
    "\n",
    "    # tokenize the entire text\n",
    "    token_ids = tokenizer.encode(text, allowed_special = {\"<|endoftext|>\"})\n",
    "    print(f\"Number of token ids : {len(token_ids)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Every journey start with one step\n",
    "    # Every -> journey \n",
    "    # Every journey -> start \n",
    "    # Every journey start -> with\n",
    "\n",
    "    # use a sliding window to chunk the book into overlapping sequence of max length\n",
    "    for i in range(0, len(token_ids) - context_window_size, stride):\n",
    "      input_chunk = token_ids[i: i+context_window_size]\n",
    "      target_chunk = token_ids[i+1: i+context_window_size + 1]\n",
    "      self.input_ids.append(torch.tensor(input_chunk))\n",
    "      self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.target_ids[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff261dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of token ids : 394637\n",
      "shape of each inputs id: 394633\n",
      "length of dataloader's input id: 394633\n"
     ]
    }
   ],
   "source": [
    "with open(\"./fiction_stories.txt\", \"r\", encoding= \"utf8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "  # preprocessed = re.split(r\"[.,:;!?()\\\"']|--|\\s\", text)\n",
    "  # preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "dataloader = GPTDatasetV1(tokenizer, text, 4, 1)\n",
    "\n",
    "# len of dataset input ids =   [len(token_id) / stride]\n",
    "print(f\"shape of each inputs id: {dataloader.__len__()}\")\n",
    "\n",
    "#length of dataloader\"s input ids is [len(token_ids) - context_window_size]\n",
    "print(f\"length of dataloader's input id: {dataloader.__len__()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d399e1",
   "metadata": {},
   "source": [
    "\n",
    "the following code will use GPTDatasetV1 to load inputs in batches via Pytorch Dataloader Step1: Initialize the tokenizer\n",
    "\n",
    "Step2: Create dataset\n",
    "\n",
    "Step3: drop_last = True, drop the last batch it it is shorter than the specified batch size to prevent loss spikes during training\n",
    "\n",
    "Step4: The number of CPU process to use for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76a5e1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# params :\n",
    "#      # num_worker = 0, Number of subprocesses to use for for data loading\n",
    "#      # pin_memory = False, if true, the data loader will copy tensors into CUDA pinned memory before returning\n",
    "#      # prefetch_factor = 2, Number of batches to prefetch\n",
    "#      # drop_last = False, if True, drop the last incomplete batch\n",
    "#      # batch size = 4, Number of batches to processes at once before adjusting weights of LLM model\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d93742d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataloader_v1(text, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_worker = 0):\n",
    "  # Initialize the tokenizer\n",
    "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "  #create dataset\n",
    "  dataset = GPTDatasetV1(tokenizer, text, max_length, stride)\n",
    "\n",
    "  # Create dataloader\n",
    "  dataloader = DataLoader(dataset,\n",
    "                          batch_size = batch_size,\n",
    "                          shuffle = shuffle,\n",
    "                          drop_last = drop_last,\n",
    "                          num_workers = num_worker)\n",
    "\n",
    "  return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d48653f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of token ids : 394637\n",
      "[tensor([[14126,   314,   198,   198]]), tensor([[ 314,  198,  198, 1268]])]\n",
      "[tensor([[ 314,  198,  198, 1268]]), tensor([[ 198,  198, 1268, 7655]])]\n",
      "Number of token ids : 394637\n",
      "inputs: \n",
      " tensor([[14126,   314,   198,   198],\n",
      "        [ 1268,  7655, 20739,  9370],\n",
      "        [41119,  1921,   376,  7730],\n",
      "        [   38,  5357,   350,  1921],\n",
      "        [ 5188, 30709, 12425, 15859],\n",
      "        [ 8905,    51,   412, 16219],\n",
      "        [25401,    11,   198, 10970],\n",
      "        [16329,  7054, 32337,  5781]])\n",
      "targets: \n",
      " tensor([[  314,   198,   198,  1268],\n",
      "        [ 7655, 20739,  9370, 41119],\n",
      "        [ 1921,   376,  7730,    38],\n",
      "        [ 5357,   350,  1921,  5188],\n",
      "        [30709, 12425, 15859,  8905],\n",
      "        [   51,   412, 16219, 25401],\n",
      "        [   11,   198, 10970, 16329],\n",
      "        [ 7054, 32337,  5781,    11]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Lets test the datalader with a batch size of 1 for an LLM with with the cotext size of 4\n",
    "# This will develop initution of how the GPTDatasetV1 class and the create dataloader v21 function work together\n",
    "\n",
    "# with open(\"/content/drive/MyDrive/Colab Notebooks/fiction_stories.txt\", \"r\", encoding= \"utf8\") as f:\n",
    "#   text = f.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(text, batch_size=1, max_length = 4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "# The first batch variable contains two tensors: the first tensor storesthe input token IDs stores the target t\n",
    "# Since the max_length is set to 4, each of two tensors contains 4 token IDs\n",
    "\n",
    "\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "\n",
    "second_batch = next(data_iter)\n",
    "print(second_batch)\n",
    "\n",
    "\n",
    "dataloader = create_dataloader_v1(text, batch_size=8, max_length = 4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, target = next(data_iter)\n",
    "\n",
    "\n",
    "print(\"inputs: \\n\", inputs)\n",
    "print(\"targets: \\n\", target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa769956",
   "metadata": {},
   "source": [
    "\n",
    "\"\"\"\n",
    "    Note that if we increase the stride to 4. This is to utilize the data set fully\n",
    "    (we dont't skip a single word more overlap between batches will lead to increased overfitting )\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
