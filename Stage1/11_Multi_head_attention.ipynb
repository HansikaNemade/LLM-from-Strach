{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d866741",
   "metadata": {},
   "source": [
    "Instead of maintaining two separate classes, Multi Head Attention Wrapper and Causal Attention, we combine both of these\n",
    "concepts into single Multi-HeadAttention class\"\"\"\n",
    "\n",
    "In the MultiHeadAttention Wrapper, multiple heads are implemented by creating a list of Causal attention Object (self.heads), \n",
    "each representing a separate attention head \n",
    "\n",
    "The CausalAttention class independently performs the attention mechansim and the results from each head are concatenated.\n",
    "\n",
    "In constrast, the following MultiHeadAttention class integrates the multi-head functionality within single class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "184da596",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ff5362a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module) :\n",
    "    def __init__(self, d_in, d_out, context_len, dropout, num_heads, qkv_bias=False) :\n",
    "        super().__init__()\n",
    "        # assert(d_out % num_heads == 0, \"d_out must be divisible by num heads\")\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_queries = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_keys = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_values = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_len, context_len), diagonal=1))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, d_in = x.shape\n",
    "        Keys = self.W_keys(x)\n",
    "        Queries = self.W_queries(x)\n",
    "        Values = self.W_values(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a 'num_heads' dimensions\n",
    "        # Unroll last dim : (batch_size, num_tokens, d_out) -> (batch_size, num_tokens, num_heads, head_dim)\n",
    "        Keys = Keys.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "        Values = Values.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "        Queries = Queries.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Tranpose : ( batch_size, num_tokens, num_heads, head_dim) -> (batch_size, num_heads, num_token, head_dim)\n",
    "        Keys = Keys.transpose(1, 2)\n",
    "        Values = Values.transpose(1, 2)\n",
    "        Queries = Queries.transpose(1, 2)\n",
    "\n",
    "        # Compute scale dot product attention (aka self attention) with causal mask\n",
    "        atten_scores = Queries @ Keys.transpose(2, 3)\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        atten_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        # Normalize causal attention scores to obtain attention weights\n",
    "        atten_weights = torch.softmax(atten_scores / (Keys.shape[-1] ** 0.5), dim=-1)\n",
    "        atten_weights = self.dropout(atten_weights)\n",
    "\n",
    "        # Shape : ( batch_Size, num_tokens, num_heads, head_dim )\n",
    "        context_vec = (atten_weights @ Values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(batch_size, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffbd6cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch shape :  torch.Size([2, 3, 6])\n",
      "context vector : \n",
      " tensor([[[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
      "         [ 0.1038, -0.0506,  0.0340, -0.0179, -0.3291, -0.2941],\n",
      "         [ 0.1145, -0.0466,  0.0280, -0.0614, -0.2814, -0.2546]],\n",
      "\n",
      "        [[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
      "         [ 0.1038, -0.0506,  0.0340, -0.0179, -0.3291, -0.2941],\n",
      "         [ 0.1145, -0.0466,  0.0280, -0.0614, -0.2814, -0.2546]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "context vector shape : \n",
      " torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Define the tensor with 3 rows and 6 columns\n",
    "inputs = torch.tensor(\n",
    "        [[0.43, 0.15, 0.89, 0.55, 0.87, 0.66],\n",
    "         [0.57, 0.85, 0.64, 0.22, 0.58, 0.45],\n",
    "         [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]\n",
    "    )\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(\"batch shape : \", batch.shape)\n",
    "\n",
    "batch_size, num_tokens, embed_dims = batch.shape \n",
    "d_out = 6\n",
    "context_len = batch.shape[1]\n",
    "mha = MultiHeadAttention(6, d_out, context_len=context_len, dropout=0.0, num_heads=2 )\n",
    "context_vec = mha(batch)\n",
    "print(\"context vector : \\n\", context_vec)\n",
    "print(\"context vector shape : \\n\", context_vec.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
